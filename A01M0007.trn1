えーっとこの研究はあの名工大の徳田先生の研究室と共同で行なってるんですがえっと八月の音声研究会で徳田先生があのー隠れマルコフモデルに基づく音声ごうあか隠れマルコフモデルへの音声合成への利用ということで特別講演をされておりむおられますのであのーその内容と重複するぐところが幾つかあるかと思いますのでもしそちらの方をお聞きになった方にはあのー御容赦いただきたいと思います
えーそれでは早速あの内容に入らしていただきます
えーとテキスト音声合成方式としてまーあのーこの前の今××さんからちょっとレビューがありましたけどもちえーいつ十年以上前まではま規則合成っていう方式がしゅゆ主流だった訳ですが最近コーパスに基づく音声合成というのがあーのー主流になってきてるとまこれも××さんの方から御紹介あったんですがまー大量の音声データーが音声データーベース良質の音声データーベースが整備されたことそれからあのー計算機の処理能力とあるいはデーターの記憶容量が大きくなったということまそれもあるんですけどもあのー規則合成がま職人芸的なノウハウを必要とするのに対してまそういうノウハウが必要なくえーシステムの自動構築がある程度できるということがまー大きな要因だと思います
後もう一つ大きな要因としてはま大量のデーターに裏付けされたあのー統計的モデル化とかあるいはえー最適な音声を素片を選択してそれを接続することによってま自然性の高い品質とい良いえー音声が合成できるということも一つの要因かと思います
でまーこういう風にそのーま大量のデーターを使うということそれから統計的なモデルを使うということでま音声合成においてもえー音声認識で使われわれてきたあのーき要素技術である隠れマルコフモデルを利用する場面が多くなってきたとえただあのーおおエイチエムエムを使う場合もま色んな場面があってまーその音声合成でどのように使われてるかっていうのは色んな場合があると思います
でま大きく分けてここでは三つ程に分類したんですけどもま一番から三番に行く程えーエイチエムエムが果たす役割がま重要になってきてるというやつです
ま一番はあのーまこう大量のデーターベースを使うということでま音声素片ラベル付けあしなきゃいけないっていうことでまトランスクリトランスクリプションとかセグメンテーションに使うとそれからま二番目はもう少し積極的に使おうということで音声素片を選択する時ま接続のコストだとかあのーま先程も話ありましたけどまメジャーを決めてそれにそのメジャーに合ったような一番いいものを取ってくる
持ってくるという時にエイチエムエムを使うとあるいはコンテキストクラスタリングを使うという
で今日お話しするのはこの三番目なんですがえっとまー我々がエイチエムエムに基づく音声合成と呼んでる内容っていうのはまずエイチエムエムによってモデル化行なってそのモデル自体エイチエムエムですけどもそのモデルからパラメーターを生成して音声を作り出すというものです
でこのエイチエムエムを音声合成に利用しようという試みは実はあのーかなり前から行なわれております
でま古いところで言うと千九百八十六年代にま音声符号化の方をてスペクトル量子化につスペクトルの量子化に使おうというような試みとかあるいはピッチパターンをうーんエイチエムエムからんえー作り出そうとま後あのー千九百八十く九年とか九十四年にも色々なまこれ以上これ以外にも色んなあのー方式があるかと思います
ただまーあのー我々の観点からするとえーまこここの辺りの研究っていうのはそのーパラメーター生成を行なう段階であのーどうしてもえーそのエイチエムエムとそのーエイチエムエムこう繋いだ時にその繋ぎのところで非常に不連続が起きてしまうんでそれをあのー線形補間とかあのー適当なスムージングを掛けてま便宜的な方法であのパラメーター生成してるという風に見える訳です
でこれに対して我々のそのエイチエムエムに基づく音声合成というのはまー幾つか特徴がある訳ですけれどけどもまその一つがえーとエイチエムエムか自体からパラメーターを生成するとでその時に動的な特徴量考慮することによってえーまこう滑らかに変化するようなま自然に近いような形のパラメーター生成が行なわれる
それからモデル化なんですがえーっと現在今実現しているうーシステムはスペクトルそれからピッチ継続長これらを同時にモデル化するようなエイチエムエムを持ってきてそれを繋いでそこからパラメーター生成行なってえー音声を生成しているとでまーその際にあのーあのーピッチの韻律情報のモデル化ってのはちょっとあスペクトルのモデル化とはあのー形態が違いますので多空間分布エイチエムエムというものを導入してえーこれを同時にモデル化が可能にしているということです
後波形生成はまメルケプストラム分析合成に基づいてでまフィルターはエムエルエスエーフィルターで使ってるとまーこれでばえーとモデル化から生成の段階までメルケプストラムというまーパラメーターを一貫して使っているということです
えーとそれではまずそのー我々のエイチエムエムに基づく音声合成のま一つのえー核となる技術でパラメーター系列生成ということをえー御説明したいと思います
んでまーこうエイチエムエムるわラムダっていうエイチエムエムがあってまこっからま何かむ尤もらしいようなこうスペクトル系列を生成したいとまそういうことを考える訳です
でまー問題の定式化としては与えられた連続分布エイチエムエムに対してえー後あの全体の時間がありますけどもまそれに対してまこの尤度と言いますか
しつあのー出力確率と言いますか
これを最大化するようなえー尤度最大化基準によってこのような尤もらしいこうスペクトル系列をえー生成したいという訳です
で例えばこのエイチエムエムを学習んっととぅことす学習する際にえーま静的特徴量のみだけを用いて学習しておいてそこからえー尤度最大化基準でスペクトルをす生成しようと思うとととこのようにまこの静的特徴量のその平均ベクトルの系列をこう並べれば一番ま尤度が高くなるという訳でこの状態から状態への遷移する段階ここでま不連続が起きてしまう訳です
でそれに対してえーっとこの観測ベクトルっていうのをま静的特徴量とそれの一次結合で表わさ表わされた動的な特徴量それん一緒に合わせたようなベクトルを考えてでこの拘束条件の下にえーこの静的特徴量に関するこの尤度の最大化を行なうとま結果的にこう滑らかに変化するようなスペクトルが出てきます
えま具体例をお見せした方がいいと思うんですが例えばえーと無音とあというモデルそれからいという母音のモデルそれから無音とこういう風な四つのこうまそれぞれ別々にいい学習しておいたエイチエムエムをこう繋いでこのエイチエムエムから先程述べたような尤度くさいでげ動的特徴量を考慮して尤度最大化基準でパラメーター系列を生成するとえーとこういう風なものになる
まあうこのシー二っていうのはえーとえーとこのエイチエムエムを学習する時にえー音響特徴量として求めたケプストラムの第二次のメルケプストラムの第二次の項です
でこのえーっとこの赤い範囲なんですがこれがちょうどえーまくそれぞれの状態はえー単一のガウス分布でえー出力分布が定義されてるんですがそれの標準偏差それからこの真ん中がちょうど平均になる訳です
で静的特徴量しか考慮しない場合はこの平均のこういう平均を並べたような出力系列になる訳ですけども動的特徴量を考慮することによって例えばえーデルタ動的特徴量の分散が大きいようなところではえーこの静的と特徴量もこうわりと大きく変化するような格好になっててでデルタの動的と特徴量の分散が小さいようなところでは静的特徴量もあんまり変化しないようなえー系列を生成するとで具体的にはスペクトルで見るとえーこのような格好です
まこれは先程言ったように静的特徴量のみを考慮するとすそれぞれの平均を並べたようながたがたのこう不連続の大きいようなスペクトル系列になるんです
ところが動的特徴量を考慮することによってこう滑らかに変化するような系列が得られるとでまずはじゃあのーこれでどのような音声が作れるのかということでま基本的な音声合成システムを作ってみました
えーまず音声データーベースからメルケプストラム分析行ないましてえーメルケプストラムとそれの動的特徴量デルタとデルタデルタを求めたいとで後は音素ごとにえーエイチエムエムを学習しまして今度は入力テキストを与えられたらそのえ音素系列に従ってえー音声エイチエムエムをこうずらっと並べて一つのおっきな一つの文章のエイチエムエムを作るとで後はここから先程い述べましたえーパラメーター系列の生成法によってメルケプストラムを出力して後それに対して適当なピッチを与えてエムエルエスエーフィルターを励振してやって合成音声とするとえーとじゃまずどんな音が出たかというのを聞いていただきます
まずはえーとエーティーアールのえー音声データーベースを使ってま男性話者一名で作った音です
えーとまず分析用ず合成音がえこれは韻律情報はあの自然音声から取ったものをそのまま使ってます
えそれに対して静的特徴量つまり平均値を並べたのみの音だとで動的特徴量を入れるとちょっとスピーカーだとあのーお分かりになりにくかったかもしれないんですがあのー接続の部分がの不連続感がちょなくなってるのが分かるかと思います
でえーま音声合成を行なうにはこのスペクトルのあモデル化だけじゃなくてぷこうピッチパターンのモデル化っていうのも非常に重要になってくる訳です
ま自然性の高いよう音を合成する為にはこのピッチパターンのこの生成をいかにしてやるかってのが非常に大きな問題となる訳ですがでまーこれをエイチエムエムでモデル化する時問題となるのがこう無声区間があるところこの部分をどうしようかということが問題になるかと思います
でそれに対してまー我々が取ったアプローチというのはんーま有声音はま一次元空間からの観測事象でえー何か連続なえー出力分布を持ってるところからのえー観測であるとえそれから無声音はゼロ次元空間からの観測事象であるとでそれがまこれむい有声音と無声音が常に同時にどっちから起こるか分からないけども同時に観測できてまーただ観測できる時はどっちか一方だけは観測できるとしてまエイチエムエムの方もえーと有声音用の分布と無声音用のえーとぶんこここっちはもうゼロ次元ですから分布はないんですけどもま二つの空間を持っていてですそれぞれの空間には重みが付いててで有声音の方でが観測しつされたらそれをいつえー適当なし出力分布でえー近似してやるとで無声音が観測されたらそのとところは重みえーだけをえー学習しておくとで実際に今度はここういうエイチエムまー我々エムエスディーエイチエムエムとゆん呼んでるんですが多空間上のエイチエムエムでモデル化しといてえーこれを実際に今度はパラメーター生成を行なう際にはえーこちらの重みが大きいか小さいかによって有声無声を区間を選んでやって有声区間であれば先程のえー動的特徴量を考慮したパラメーター生成法によってえーピッチパターンを生成してやるとで無声区間の方が確率が高ければそこは無声としてやるというようなあー生成方法を取っています
で具体的に今のおーあのー同時にモデル化したえーとピッつスペクトルとピッチとそれから継続長も一緒にモデル化したあーエイチエムエムを使った合成システムのえーこれはブロック図です
えーとそまここでそれぞれえーとま音声単位にす相当するえーエイチエムエムを用意しておいおきます
でこの場合はこの音声単位に相当するのはコンテキスト依存モデルと言いましてまー基本的には音素なんですがえーと色んな一つの音素でも例えばアクセント核があるとかあるいはモーラのどの辺モーラ位置によるとかあるいは前がどんなぼあえー音素であるとかか色んな要因がある訳ですからえーとそれのそれらの要因を全て考えたようなえーコンテキスト依存モデル作っておいまず最初に学習でしておいて作っておくとで後はテキストが入ってきたらそのコンテキストベーストのラベルシークエンスにしてそれをずっと並べてそこからパラメーター生成を行なってえー後は合成するとでまでこのコンテキスト依存モデルなんですがあのーコンテキストというのはえートライホンつっあ先行音素と後続音素だけ考えただけでももうかなり膨大な数で更にピッチのことをあるいは品詞とか品詞情報とかえーモーラ位置情報とか文の長さの情報とか全部考えると物凄い膨大な量になりますのでえーそれぞれに対応したえーモデルを全部用意するするのはちょっと不可能実現不可能そこでどうやってるかと言うとまコンテキストクラスタリングを使いましてえーとそれぞれスペクトルあるいはピッチあるいは継続長でえーディシジョンツリーを作って決定木をつ作っておきましてでま学習データーにないようなコンテキストが現われても必ずどこかのどっかからえー引っ張ってこれるという風な形を取ってる
でこのこういうコンテキストいぞ依存モデルを使ううあのー合成法ってのはまー我々のところじゃだけではなくてえー例えばマイクロソフトとかあるいはえーケンブリッジからあのー今アイビーエムに行ってますドノバンという人がやってますけれどもすその合成方式でもやはりコンテキストえークラスタリングを使って決定木を作ってで我々のところはこのはっ一番あのーえーと先のリーフのところにこういう分布を持ってるんですけどもあのーマイクロソフトとかアイビーエムでやってるのはえーその色んなインスタンスを持っててそん中から一番いいのを選んでくるというような格好になってます
じゃえーと合成例をどんな合成例合成音声ができるのかっていうのを聞いていただきたいと思います
えーっと音声データーベースはエーティーアールの音韻バランス文四百五十文でえーっとまー二十四次のゼロから二十四次までのメルケプストラムとそれとデルタデルタデルタそれからピッチ周波数とそのデルタデルタデルタま七十八次元で音響パラメーター構築しておいてそれから継続長は五次元のガウス分布後コンテキストクラスタリングを掛けてスペクトルが九百四十三分布ピッチが二千四百五十二ん分布継続長が五百七十九分布でえーツリーを作っています
で後この音はあのー韻律情報も含めて全てあのーしえーとこのシステムで作った音です
でまーこれはその時出てきたスペクトルそれからピッチでまーこの後でまたあのースペシャルセッションの後の講演の方でありますがまーあのーピッチのこう生成の方も静的特徴のみだとこういうがたがたになるんですが結構でここう動的ちょ特徴量を使うとわりと滑らかなえあのーピッチパターンが得られるとそれからあまた言い忘れたんですが今うこれらの例出す例はこの四百五十文に含まれていない学習データーには含まれてない文章失礼しました
えーとまえんつこのエイチエムエム音声合成はまこれだけで終わらなくて実は非常に広い応用が考えられます
例えば話者補間に基づく多様な音声の声質の実現話者適応に基づく声質変換エイチエムエム認識ボコーダー合成音声による詐称バイモーダル音声合成とま色々あるんですが今日はもう時間がないのでちょっとこの三つだけえー紹介したいと思います
まず話者適応にかよる声質変換なんですがえーっと今御説明した音声合成システムっていうのはこう音声データーベースとっおが決まるとその話者のまーあのーエイチエムエムを学習してでそこから音出しますからその人の学習したデーターベースにある人の声しか出ない訳ですね
ところがまー合成の応用を考えるとま色んな人の声で出したいという要望が強い訳です
でうそうするにはまー色んな人の話者のうあのーモデルをいっぱい持ってけばいいんですがま現実的にはそれは不可能なんでまず不特定話者のモデルを作っておいてそれをま少量の特定話者のデーターを持ってきて後は話者適応を掛けてやってこのエイチエムエムをその話者に適応させてでその適応したえーエイチエムエムを繋いでパラメーター生成を行なうとこういうことによってあのー色んな人の声が出せるとえ実際に作った例なんですがまずま本来特定話者のモデルだとまこういうエムあのこれはエーティーアールのデンタデーターベースのエムワイアイさんの声なんですけどもこういう声でおん合成したいんですがえーとエムワイアイさんを含まない不特定話者この場合は十名の話者を持ってきて不特定話者のエイチエムエムを作ると作って音を出す
作るとちょちょっとエムワイアイさんの声とは全然違う声になってると思います
だからこれよよに五文章でエムエルエルアールでえー話者適応を掛けるとどんな声になるかと言うとかなりこのとっ特定話者モデルの声につ近い声になってるというのが分かるかと思います
えーとだいぶ時間がもう押してきたんですが後は認識ボコーダーこれはあえー入力音声を音声認識を掛けてピッチと音声おんしょ音素系列とえーじぞ継続時間だけを送るということでまー非常に数百ビットぐらいというあー数百ビット・パー・セックという非非常に低いレーレートで音声の符号化が行なえるという訳ですがえこれもあのーあの同じここで持ってるえーおエイチエムエムの人の声しか出せないという訳でこれもやはりえー適応を掛けることによって色んな人の声を出したいとでこれはその例なんですが例えば入力音声としてこういう声が入ってきた時にえーと不特定話者モデルで音を作るとま話者性が殆ど失われてるんですがそれに対してま百ビット・パー・セックぐらいで話者情報送ってやることによっておっとごめんなさい
こっちでしたね
まかなりあのー元の話者性に近い音が出てるかと後例の最後ですがえーと話者照照合システムの安全性ま今までそのー話者照合システムの安全性っていうのはあんまり検討されてなかったと言うかあの人間の詐称者に対してはま尤度正規化等の方法を取ればまー殆ど大丈夫だよというようなことが言われて後録音音声に対してはまテキスト指定型をのシステム作れば大丈夫だよというようなことが言われてむ合成音声に対してはあまり検討されてなかったんですがま最近合成音声に対してどういうどれくらい安全かっていうのがあのーここんい去年のあえアイシーエスエルピーとか今年のアアイキャスプユーロスピーチでもあのー他の研究者が何件か発表してましたけどもま合成音声のおー質が良くなってきたんでまこれも検討する必要があるだろうとでま我々がやった実験ていうのはあのーわテキスト指定型でえーとまー条件としてはその合成側と認識側の条件が違ってた方が現実的だろうということで合成側はメルケプストラムで合成してえーと認識側の話者照合システムの認識側の方はエルピーシーケプストラムでえー音響パラメーターを取って照合したとで実際には元はこういうまこういう音を入れるとまこの人はそうだよということなんですがま我々がやった実験でまー実は合成音ピッチ情報全部落としちゃって全部ぬいはっ白色ノイズであのー励振したような音を使ってその詐称の実験をやってみました
そうしたら驚くことにえーとベースラインのその詐称率は零パーセントだったのがえーとま二文章とか三文章四文章ぐらいのデーターを持ってきて適応掛けて先程の音声合成掛けるとん七八はっ七七割から八割ぐらいの詐称率になってしまうということでまーこれももう少しえ照合システム話者照合システムのえーこういう合成音声に対して強いようなあーシステムを作らなきゃいけないということだと思います
えーとちょっと時間がもう来てしまったんですがまエイチエムエムに基づく音声合成とひいうことをあー簡単に紹介しました
えーっとまー一番の特徴はえーっと多様な音声が作れるということじゃないかと思います
ただまー現時点のこの合成音声はあのボコーダーに基づいてますので分析合成に単純な分析合成に基づいてますのでま必ずしもそのーそのー高品質な音とは言えないかもしれないんですがま今後はちょっと音源等を工夫してもう少しあの音質を上げていきたいと考えております
以上です
